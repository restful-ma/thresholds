openapi: 3.0.0
info:
  version: 2015-08-04
  x-release: v4
  title: Amazon Kinesis Firehose
  description: <fullname>Amazon Kinesis Data Firehose API Reference</fullname> <p>Amazon
    Kinesis Data Firehose is a fully managed service that delivers real-time
    streaming data to destinations such as Amazon Simple Storage Service (Amazon
    S3), Amazon Elasticsearch Service (Amazon ES), Amazon Redshift, and
    Splunk.</p>
  x-logo:
    url: https://twitter.com/awscloud/profile_image?size=original
    backgroundColor: "#FFFFFF"
  termsOfService: https://aws.amazon.com/service-terms/
  contact:
    name: Mike Ralphson
    email: mike.ralphson@gmail.com
    url: https://github.com/mermade/aws2openapi
    x-twitter: PermittedSoc
  license:
    name: Apache 2.0 License
    url: http://www.apache.org/licenses/
  x-providerName: amazonaws.com
  x-serviceName: firehose
  x-origin:
    - contentType: application/json
      url: https://raw.githubusercontent.com/aws/aws-sdk-js/master/apis/firehose-2015-08-04.normal.json
      converter:
        url: https://github.com/mermade/aws2openapi
        version: 1.0.0
      x-apisguru-direct: true
  x-apiClientRegistration:
    url: https://portal.aws.amazon.com/gp/aws/developer/registration/index.html?nc2=h_ct
  x-apisguru-categories:
    - cloud
  x-preferred: true
externalDocs:
  description: Amazon Web Services documentation
  url: https://docs.aws.amazon.com/firehose/
x-hasEquivalentPaths: true
security:
  - hmac: []
paths:
  /#X-Amz-Target=Firehose_20150804.CreateDeliveryStream:
    post:
      operationId: CreateDeliveryStream
      description: "<p>Creates a Kinesis Data Firehose delivery stream.</p> <p>By default,
        you can create up to 50 delivery streams per AWS Region.</p> <p>This is
        an asynchronous operation that immediately returns. The initial status
        of the delivery stream is <code>CREATING</code>. After the delivery
        stream is created, its status is <code>ACTIVE</code> and it now accepts
        data. Attempts to send data to a delivery stream that is not in the
        <code>ACTIVE</code> state cause an exception. To check the state of a
        delivery stream, use <a>DescribeDeliveryStream</a>.</p> <p>A Kinesis
        Data Firehose delivery stream can be configured to receive records
        directly from providers using <a>PutRecord</a> or <a>PutRecordBatch</a>,
        or it can be configured to use an existing Kinesis stream as its source.
        To specify a Kinesis data stream as input, set the
        <code>DeliveryStreamType</code> parameter to
        <code>KinesisStreamAsSource</code>, and provide the Kinesis stream
        Amazon Resource Name (ARN) and role ARN in the
        <code>KinesisStreamSourceConfiguration</code> parameter.</p> <p>A
        delivery stream is configured with a single destination: Amazon S3,
        Amazon ES, Amazon Redshift, or Splunk. You must specify only one of the
        following destination configuration parameters:
        <code>ExtendedS3DestinationConfiguration</code>,
        <code>S3DestinationConfiguration</code>,
        <code>ElasticsearchDestinationConfiguration</code>,
        <code>RedshiftDestinationConfiguration</code>, or
        <code>SplunkDestinationConfiguration</code>.</p> <p>When you specify
        <code>S3DestinationConfiguration</code>, you can also provide the
        following optional values: BufferingHints,
        <code>EncryptionConfiguration</code>, and
        <code>CompressionFormat</code>. By default, if no
        <code>BufferingHints</code> value is provided, Kinesis Data Firehose
        buffers data up to 5 MB or for 5 minutes, whichever condition is
        satisfied first. <code>BufferingHints</code> is a hint, so there are
        some cases where the service cannot adhere to these conditions strictly.
        For example, record boundaries might be such that the size is a little
        over or under the configured buffering size. By default, no encryption
        is performed. We strongly recommend that you enable encryption to ensure
        secure data storage in Amazon S3.</p> <p>A few notes about Amazon
        Redshift as a destination:</p> <ul> <li> <p>An Amazon Redshift
        destination requires an S3 bucket as intermediate location. Kinesis Data
        Firehose first delivers data to Amazon S3 and then uses
        <code>COPY</code> syntax to load data into an Amazon Redshift table.
        This is specified in the
        <code>RedshiftDestinationConfiguration.S3Configuration</code>
        parameter.</p> </li> <li> <p>The compression formats <code>SNAPPY</code>
        or <code>ZIP</code> cannot be specified in
        <code>RedshiftDestinationConfiguration.S3Configuration</code> because
        the Amazon Redshift <code>COPY</code> operation that reads from the S3
        bucket doesn't support these compression formats.</p> </li> <li> <p>We
        strongly recommend that you use the user name and password you provide
        exclusively with Kinesis Data Firehose, and that the permissions for the
        account are restricted for Amazon Redshift <code>INSERT</code>
        permissions.</p> </li> </ul> <p>Kinesis Data Firehose assumes the IAM
        role that is configured as part of the destination. The role should
        allow the Kinesis Data Firehose principal to assume the role, and the
        role should have permissions that allow the service to deliver the data.
        For more information, see <a
        href=\"http://docs.aws.amazon.com/firehose/latest/dev/controlling-acces\
        s.html#using-iam-s3\">Grant Kinesis Data Firehose Access to an Amazon S3
        Destination</a> in the <i>Amazon Kinesis Data Firehose Developer
        Guide</i>.</p>"
      responses:
        "200":
          description: Success
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/CreateDeliveryStreamOutput"
        "480":
          description: InvalidArgumentException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/InvalidArgumentException"
        "481":
          description: LimitExceededException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/LimitExceededException"
        "482":
          description: ResourceInUseException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ResourceInUseException"
      parameters:
        - name: X-Amz-Target
          in: header
          required: true
          schema:
            type: string
            enum:
              - Firehose_20150804.CreateDeliveryStream
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateDeliveryStreamInput"
        required: true
    parameters:
      - $ref: "#/components/parameters/X-Amz-Content-Sha256"
      - $ref: "#/components/parameters/X-Amz-Date"
      - $ref: "#/components/parameters/X-Amz-Algorithm"
      - $ref: "#/components/parameters/X-Amz-Credential"
      - $ref: "#/components/parameters/X-Amz-Security-Token"
      - $ref: "#/components/parameters/X-Amz-Signature"
      - $ref: "#/components/parameters/X-Amz-SignedHeaders"
  /#X-Amz-Target=Firehose_20150804.DeleteDeliveryStream:
    post:
      operationId: DeleteDeliveryStream
      description: <p>Deletes a delivery stream and its data.</p> <p>You can delete a
        delivery stream only if it is in <code>ACTIVE</code> or
        <code>DELETING</code> state, and not in the <code>CREATING</code> state.
        While the deletion request is in process, the delivery stream is in the
        <code>DELETING</code> state.</p> <p>To check the state of a delivery
        stream, use <a>DescribeDeliveryStream</a>.</p> <p>While the delivery
        stream is <code>DELETING</code> state, the service might continue to
        accept the records, but it doesn't make any guarantees with respect to
        delivering the data. Therefore, as a best practice, you should first
        stop any applications that are sending records before deleting a
        delivery stream.</p>
      responses:
        "200":
          description: Success
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/DeleteDeliveryStreamOutput"
        "480":
          description: ResourceInUseException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ResourceInUseException"
        "481":
          description: ResourceNotFoundException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ResourceNotFoundException"
      parameters:
        - name: X-Amz-Target
          in: header
          required: true
          schema:
            type: string
            enum:
              - Firehose_20150804.DeleteDeliveryStream
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/DeleteDeliveryStreamInput"
        required: true
    parameters:
      - $ref: "#/components/parameters/X-Amz-Content-Sha256"
      - $ref: "#/components/parameters/X-Amz-Date"
      - $ref: "#/components/parameters/X-Amz-Algorithm"
      - $ref: "#/components/parameters/X-Amz-Credential"
      - $ref: "#/components/parameters/X-Amz-Security-Token"
      - $ref: "#/components/parameters/X-Amz-Signature"
      - $ref: "#/components/parameters/X-Amz-SignedHeaders"
  /#X-Amz-Target=Firehose_20150804.DescribeDeliveryStream:
    post:
      operationId: DescribeDeliveryStream
      description: Describes the specified delivery stream and gets the status. For
        example, after your delivery stream is created, call
        <code>DescribeDeliveryStream</code> to see whether the delivery stream
        is <code>ACTIVE</code> and therefore ready for data to be sent to it.
      responses:
        "200":
          description: Success
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/DescribeDeliveryStreamOutput"
        "480":
          description: ResourceNotFoundException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ResourceNotFoundException"
      parameters:
        - name: X-Amz-Target
          in: header
          required: true
          schema:
            type: string
            enum:
              - Firehose_20150804.DescribeDeliveryStream
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/DescribeDeliveryStreamInput"
        required: true
    parameters:
      - $ref: "#/components/parameters/X-Amz-Content-Sha256"
      - $ref: "#/components/parameters/X-Amz-Date"
      - $ref: "#/components/parameters/X-Amz-Algorithm"
      - $ref: "#/components/parameters/X-Amz-Credential"
      - $ref: "#/components/parameters/X-Amz-Security-Token"
      - $ref: "#/components/parameters/X-Amz-Signature"
      - $ref: "#/components/parameters/X-Amz-SignedHeaders"
  /#X-Amz-Target=Firehose_20150804.ListDeliveryStreams:
    post:
      operationId: ListDeliveryStreams
      description: <p>Lists your delivery streams in alphabetical order of their names.</p>
        <p>The number of delivery streams might be too large to return using a
        single call to <code>ListDeliveryStreams</code>. You can limit the
        number of delivery streams returned, using the <code>Limit</code>
        parameter. To determine whether there are more delivery streams to list,
        check the value of <code>HasMoreDeliveryStreams</code> in the output. If
        there are more delivery streams to list, you can request them by calling
        this operation again and setting the
        <code>ExclusiveStartDeliveryStreamName</code> parameter to the name of
        the last delivery stream returned in the last call.</p>
      responses:
        "200":
          description: Success
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ListDeliveryStreamsOutput"
      parameters:
        - name: X-Amz-Target
          in: header
          required: true
          schema:
            type: string
            enum:
              - Firehose_20150804.ListDeliveryStreams
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/ListDeliveryStreamsInput"
        required: true
    parameters:
      - $ref: "#/components/parameters/X-Amz-Content-Sha256"
      - $ref: "#/components/parameters/X-Amz-Date"
      - $ref: "#/components/parameters/X-Amz-Algorithm"
      - $ref: "#/components/parameters/X-Amz-Credential"
      - $ref: "#/components/parameters/X-Amz-Security-Token"
      - $ref: "#/components/parameters/X-Amz-Signature"
      - $ref: "#/components/parameters/X-Amz-SignedHeaders"
  /#X-Amz-Target=Firehose_20150804.ListTagsForDeliveryStream:
    post:
      operationId: ListTagsForDeliveryStream
      description: "Lists the tags for the specified delivery stream. This operation has a
        limit of five transactions per second per account. "
      responses:
        "200":
          description: Success
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ListTagsForDeliveryStreamOutput"
        "480":
          description: ResourceNotFoundException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ResourceNotFoundException"
        "481":
          description: InvalidArgumentException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/InvalidArgumentException"
        "482":
          description: LimitExceededException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/LimitExceededException"
      parameters:
        - name: X-Amz-Target
          in: header
          required: true
          schema:
            type: string
            enum:
              - Firehose_20150804.ListTagsForDeliveryStream
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/ListTagsForDeliveryStreamInput"
        required: true
    parameters:
      - $ref: "#/components/parameters/X-Amz-Content-Sha256"
      - $ref: "#/components/parameters/X-Amz-Date"
      - $ref: "#/components/parameters/X-Amz-Algorithm"
      - $ref: "#/components/parameters/X-Amz-Credential"
      - $ref: "#/components/parameters/X-Amz-Security-Token"
      - $ref: "#/components/parameters/X-Amz-Signature"
      - $ref: "#/components/parameters/X-Amz-SignedHeaders"
  /#X-Amz-Target=Firehose_20150804.PutRecord:
    post:
      operationId: PutRecord
      description: <p>Writes a single data record into an Amazon Kinesis Data Firehose
        delivery stream. To write multiple data records into a delivery stream,
        use <a>PutRecordBatch</a>. Applications using these operations are
        referred to as producers.</p> <p>By default, each delivery stream can
        take in up to 2,000 transactions per second, 5,000 records per second,
        or 5 MB per second. If you use <a>PutRecord</a> and
        <a>PutRecordBatch</a>, the limits are an aggregate across these two
        operations for each delivery stream. For more information about limits
        and how to request an increase, see <a
        href="http://docs.aws.amazon.com/firehose/latest/dev/limits.html">Amazon
        Kinesis Data Firehose Limits</a>. </p> <p>You must specify the name of
        the delivery stream and the data record when using <a>PutRecord</a>. The
        data record consists of a data blob that can be up to 1,000 KB in size,
        and any kind of data. For example, it can be a segment from a log file,
        geographic location data, website clickstream data, and so on.</p>
        <p>Kinesis Data Firehose buffers records before delivering them to the
        destination. To disambiguate the data blobs at the destination, a common
        solution is to use delimiters in the data, such as a newline
        (<code>\n</code>) or some other character unique within the data. This
        allows the consumer application to parse individual data items when
        reading the data from the destination.</p> <p>The <code>PutRecord</code>
        operation returns a <code>RecordId</code>, which is a unique string
        assigned to each record. Producer applications can use this ID for
        purposes such as auditability and investigation.</p> <p>If the
        <code>PutRecord</code> operation throws a
        <code>ServiceUnavailableException</code>, back off and retry. If the
        exception persists, it is possible that the throughput limits have been
        exceeded for the delivery stream. </p> <p>Data records sent to Kinesis
        Data Firehose are stored for 24 hours from the time they are added to a
        delivery stream as it tries to send the records to the destination. If
        the destination is unreachable for more than 24 hours, the data is no
        longer available.</p> <important> <p>Don't concatenate two or more
        base64 strings to form the data fields of your records. Instead,
        concatenate the raw data, then perform base64 encoding.</p> </important>
      responses:
        "200":
          description: Success
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/PutRecordOutput"
        "480":
          description: ResourceNotFoundException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ResourceNotFoundException"
        "481":
          description: InvalidArgumentException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/InvalidArgumentException"
        "482":
          description: ServiceUnavailableException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ServiceUnavailableException"
      parameters:
        - name: X-Amz-Target
          in: header
          required: true
          schema:
            type: string
            enum:
              - Firehose_20150804.PutRecord
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/PutRecordInput"
        required: true
    parameters:
      - $ref: "#/components/parameters/X-Amz-Content-Sha256"
      - $ref: "#/components/parameters/X-Amz-Date"
      - $ref: "#/components/parameters/X-Amz-Algorithm"
      - $ref: "#/components/parameters/X-Amz-Credential"
      - $ref: "#/components/parameters/X-Amz-Security-Token"
      - $ref: "#/components/parameters/X-Amz-Signature"
      - $ref: "#/components/parameters/X-Amz-SignedHeaders"
  /#X-Amz-Target=Firehose_20150804.PutRecordBatch:
    post:
      operationId: PutRecordBatch
      description: "<p>Writes multiple data records into a delivery stream in a single
        call, which can achieve higher throughput per producer than when writing
        single records. To write single data records into a delivery stream, use
        <a>PutRecord</a>. Applications using these operations are referred to as
        producers.</p> <p>By default, each delivery stream can take in up to
        2,000 transactions per second, 5,000 records per second, or 5 MB per
        second. If you use <a>PutRecord</a> and <a>PutRecordBatch</a>, the
        limits are an aggregate across these two operations for each delivery
        stream. For more information about limits, see <a
        href=\"http://docs.aws.amazon.com/firehose/latest/dev/limits.html\">Ama\
        zon Kinesis Data Firehose Limits</a>.</p> <p>Each <a>PutRecordBatch</a>
        request supports up to 500 records. Each record in the request can be as
        large as 1,000 KB (before 64-bit encoding), up to a limit of 4 MB for
        the entire request. These limits cannot be changed.</p> <p>You must
        specify the name of the delivery stream and the data record when using
        <a>PutRecord</a>. The data record consists of a data blob that can be up
        to 1,000 KB in size, and any kind of data. For example, it could be a
        segment from a log file, geographic location data, website clickstream
        data, and so on.</p> <p>Kinesis Data Firehose buffers records before
        delivering them to the destination. To disambiguate the data blobs at
        the destination, a common solution is to use delimiters in the data,
        such as a newline (<code>\\n</code>) or some other character unique
        within the data. This allows the consumer application to parse
        individual data items when reading the data from the destination.</p>
        <p>The <a>PutRecordBatch</a> response includes a count of failed
        records, <code>FailedPutCount</code>, and an array of responses,
        <code>RequestResponses</code>. Even if the <a>PutRecordBatch</a> call
        succeeds, the value of <code>FailedPutCount</code> may be greater than
        0, indicating that there are records for which the operation didn't
        succeed. Each entry in the <code>RequestResponses</code> array provides
        additional information about the processed record. It directly
        correlates with a record in the request array using the same ordering,
        from the top to the bottom. The response array always includes the same
        number of records as the request array. <code>RequestResponses</code>
        includes both successfully and unsuccessfully processed records. Kinesis
        Data Firehose tries to process all records in each <a>PutRecordBatch</a>
        request. A single record failure does not stop the processing of
        subsequent records. </p> <p>A successfully processed record includes a
        <code>RecordId</code> value, which is unique for the record. An
        unsuccessfully processed record includes <code>ErrorCode</code> and
        <code>ErrorMessage</code> values. <code>ErrorCode</code> reflects the
        type of error, and is one of the following values:
        <code>ServiceUnavailableException</code> or
        <code>InternalFailure</code>. <code>ErrorMessage</code> provides more
        detailed information about the error.</p> <p>If there is an internal
        server error or a timeout, the write might have completed or it might
        have failed. If <code>FailedPutCount</code> is greater than 0, retry the
        request, resending only those records that might have failed processing.
        This minimizes the possible duplicate records and also reduces the total
        bytes sent (and corresponding charges). We recommend that you handle any
        duplicates at the destination.</p> <p>If <a>PutRecordBatch</a> throws
        <code>ServiceUnavailableException</code>, back off and retry. If the
        exception persists, it is possible that the throughput limits have been
        exceeded for the delivery stream.</p> <p>Data records sent to Kinesis
        Data Firehose are stored for 24 hours from the time they are added to a
        delivery stream as it attempts to send the records to the destination.
        If the destination is unreachable for more than 24 hours, the data is no
        longer available.</p> <important> <p>Don't concatenate two or more
        base64 strings to form the data fields of your records. Instead,
        concatenate the raw data, then perform base64 encoding.</p>
        </important>"
      responses:
        "200":
          description: Success
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/PutRecordBatchOutput"
        "480":
          description: ResourceNotFoundException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ResourceNotFoundException"
        "481":
          description: InvalidArgumentException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/InvalidArgumentException"
        "482":
          description: ServiceUnavailableException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ServiceUnavailableException"
      parameters:
        - name: X-Amz-Target
          in: header
          required: true
          schema:
            type: string
            enum:
              - Firehose_20150804.PutRecordBatch
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/PutRecordBatchInput"
        required: true
    parameters:
      - $ref: "#/components/parameters/X-Amz-Content-Sha256"
      - $ref: "#/components/parameters/X-Amz-Date"
      - $ref: "#/components/parameters/X-Amz-Algorithm"
      - $ref: "#/components/parameters/X-Amz-Credential"
      - $ref: "#/components/parameters/X-Amz-Security-Token"
      - $ref: "#/components/parameters/X-Amz-Signature"
      - $ref: "#/components/parameters/X-Amz-SignedHeaders"
  /#X-Amz-Target=Firehose_20150804.StartDeliveryStreamEncryption:
    post:
      operationId: StartDeliveryStreamEncryption
      description: <p>Enables server-side encryption (SSE) for the delivery stream. </p>
        <p>This operation is asynchronous. It returns immediately. When you
        invoke it, Kinesis Data Firehose first sets the status of the stream to
        <code>ENABLING</code>, and then to <code>ENABLED</code>. You can
        continue to read and write data to your stream while its status is
        <code>ENABLING</code>, but the data is not encrypted. It can take up to
        5 seconds after the encryption status changes to <code>ENABLED</code>
        before all records written to the delivery stream are encrypted. To find
        out whether a record or a batch of records was encrypted, check the
        response elements <a>PutRecordOutput$Encrypted</a> and
        <a>PutRecordBatchOutput$Encrypted</a>, respectively.</p> <p>To check the
        encryption state of a delivery stream, use
        <a>DescribeDeliveryStream</a>.</p> <p>You can only enable SSE for a
        delivery stream that uses <code>DirectPut</code> as its source. </p>
        <p>The <code>StartDeliveryStreamEncryption</code> and
        <code>StopDeliveryStreamEncryption</code> operations have a combined
        limit of 25 calls per delivery stream per 24 hours. For example, you
        reach the limit if you call <code>StartDeliveryStreamEncryption</code>
        13 times and <code>StopDeliveryStreamEncryption</code> 12 times for the
        same delivery stream in a 24-hour period.</p>
      responses:
        "200":
          description: Success
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/StartDeliveryStreamEncryptionOutput"
        "480":
          description: ResourceNotFoundException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ResourceNotFoundException"
        "481":
          description: ResourceInUseException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ResourceInUseException"
        "482":
          description: InvalidArgumentException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/InvalidArgumentException"
        "483":
          description: LimitExceededException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/LimitExceededException"
      parameters:
        - name: X-Amz-Target
          in: header
          required: true
          schema:
            type: string
            enum:
              - Firehose_20150804.StartDeliveryStreamEncryption
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/StartDeliveryStreamEncryptionInput"
        required: true
    parameters:
      - $ref: "#/components/parameters/X-Amz-Content-Sha256"
      - $ref: "#/components/parameters/X-Amz-Date"
      - $ref: "#/components/parameters/X-Amz-Algorithm"
      - $ref: "#/components/parameters/X-Amz-Credential"
      - $ref: "#/components/parameters/X-Amz-Security-Token"
      - $ref: "#/components/parameters/X-Amz-Signature"
      - $ref: "#/components/parameters/X-Amz-SignedHeaders"
  /#X-Amz-Target=Firehose_20150804.StopDeliveryStreamEncryption:
    post:
      operationId: StopDeliveryStreamEncryption
      description: <p>Disables server-side encryption (SSE) for the delivery stream. </p>
        <p>This operation is asynchronous. It returns immediately. When you
        invoke it, Kinesis Data Firehose first sets the status of the stream to
        <code>DISABLING</code>, and then to <code>DISABLED</code>. You can
        continue to read and write data to your stream while its status is
        <code>DISABLING</code>. It can take up to 5 seconds after the encryption
        status changes to <code>DISABLED</code> before all records written to
        the delivery stream are no longer subject to encryption. To find out
        whether a record or a batch of records was encrypted, check the response
        elements <a>PutRecordOutput$Encrypted</a> and
        <a>PutRecordBatchOutput$Encrypted</a>, respectively.</p> <p>To check the
        encryption state of a delivery stream, use
        <a>DescribeDeliveryStream</a>. </p> <p>The
        <code>StartDeliveryStreamEncryption</code> and
        <code>StopDeliveryStreamEncryption</code> operations have a combined
        limit of 25 calls per delivery stream per 24 hours. For example, you
        reach the limit if you call <code>StartDeliveryStreamEncryption</code>
        13 times and <code>StopDeliveryStreamEncryption</code> 12 times for the
        same delivery stream in a 24-hour period.</p>
      responses:
        "200":
          description: Success
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/StopDeliveryStreamEncryptionOutput"
        "480":
          description: ResourceNotFoundException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ResourceNotFoundException"
        "481":
          description: ResourceInUseException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ResourceInUseException"
        "482":
          description: InvalidArgumentException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/InvalidArgumentException"
        "483":
          description: LimitExceededException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/LimitExceededException"
      parameters:
        - name: X-Amz-Target
          in: header
          required: true
          schema:
            type: string
            enum:
              - Firehose_20150804.StopDeliveryStreamEncryption
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/StopDeliveryStreamEncryptionInput"
        required: true
    parameters:
      - $ref: "#/components/parameters/X-Amz-Content-Sha256"
      - $ref: "#/components/parameters/X-Amz-Date"
      - $ref: "#/components/parameters/X-Amz-Algorithm"
      - $ref: "#/components/parameters/X-Amz-Credential"
      - $ref: "#/components/parameters/X-Amz-Security-Token"
      - $ref: "#/components/parameters/X-Amz-Signature"
      - $ref: "#/components/parameters/X-Amz-SignedHeaders"
  /#X-Amz-Target=Firehose_20150804.TagDeliveryStream:
    post:
      operationId: TagDeliveryStream
      description: <p>Adds or updates tags for the specified delivery stream. A tag is a
        key-value pair that you can define and assign to AWS resources. If you
        specify a tag that already exists, the tag value is replaced with the
        value that you specify in the request. Tags are metadata. For example,
        you can add friendly names and descriptions or other types of
        information that can help you distinguish the delivery stream. For more
        information about tags, see <a
        href="https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html">Using
        Cost Allocation Tags</a> in the <i>AWS Billing and Cost Management User
        Guide</i>. </p> <p>Each delivery stream can have up to 50 tags. </p>
        <p>This operation has a limit of five transactions per second per
        account. </p>
      responses:
        "200":
          description: Success
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/TagDeliveryStreamOutput"
        "480":
          description: ResourceNotFoundException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ResourceNotFoundException"
        "481":
          description: ResourceInUseException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ResourceInUseException"
        "482":
          description: InvalidArgumentException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/InvalidArgumentException"
        "483":
          description: LimitExceededException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/LimitExceededException"
      parameters:
        - name: X-Amz-Target
          in: header
          required: true
          schema:
            type: string
            enum:
              - Firehose_20150804.TagDeliveryStream
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/TagDeliveryStreamInput"
        required: true
    parameters:
      - $ref: "#/components/parameters/X-Amz-Content-Sha256"
      - $ref: "#/components/parameters/X-Amz-Date"
      - $ref: "#/components/parameters/X-Amz-Algorithm"
      - $ref: "#/components/parameters/X-Amz-Credential"
      - $ref: "#/components/parameters/X-Amz-Security-Token"
      - $ref: "#/components/parameters/X-Amz-Signature"
      - $ref: "#/components/parameters/X-Amz-SignedHeaders"
  /#X-Amz-Target=Firehose_20150804.UntagDeliveryStream:
    post:
      operationId: UntagDeliveryStream
      description: <p>Removes tags from the specified delivery stream. Removed tags are
        deleted, and you can't recover them after this operation successfully
        completes.</p> <p>If you specify a tag that doesn't exist, the operation
        ignores it.</p> <p>This operation has a limit of five transactions per
        second per account. </p>
      responses:
        "200":
          description: Success
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/UntagDeliveryStreamOutput"
        "480":
          description: ResourceNotFoundException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ResourceNotFoundException"
        "481":
          description: ResourceInUseException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ResourceInUseException"
        "482":
          description: InvalidArgumentException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/InvalidArgumentException"
        "483":
          description: LimitExceededException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/LimitExceededException"
      parameters:
        - name: X-Amz-Target
          in: header
          required: true
          schema:
            type: string
            enum:
              - Firehose_20150804.UntagDeliveryStream
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/UntagDeliveryStreamInput"
        required: true
    parameters:
      - $ref: "#/components/parameters/X-Amz-Content-Sha256"
      - $ref: "#/components/parameters/X-Amz-Date"
      - $ref: "#/components/parameters/X-Amz-Algorithm"
      - $ref: "#/components/parameters/X-Amz-Credential"
      - $ref: "#/components/parameters/X-Amz-Security-Token"
      - $ref: "#/components/parameters/X-Amz-Signature"
      - $ref: "#/components/parameters/X-Amz-SignedHeaders"
  /#X-Amz-Target=Firehose_20150804.UpdateDestination:
    post:
      operationId: UpdateDestination
      description: <p>Updates the specified destination of the specified delivery
        stream.</p> <p>Use this operation to change the destination type (for
        example, to replace the Amazon S3 destination with Amazon Redshift) or
        change the parameters associated with a destination (for example, to
        change the bucket name of the Amazon S3 destination). The update might
        not occur immediately. The target delivery stream remains active while
        the configurations are updated, so data writes to the delivery stream
        can continue during this process. The updated configurations are usually
        effective within a few minutes.</p> <p>Switching between Amazon ES and
        other services is not supported. For an Amazon ES destination, you can
        only update to another Amazon ES destination.</p> <p>If the destination
        type is the same, Kinesis Data Firehose merges the configuration
        parameters specified with the destination configuration that already
        exists on the delivery stream. If any of the parameters are not
        specified in the call, the existing values are retained. For example, in
        the Amazon S3 destination, if <a>EncryptionConfiguration</a> is not
        specified, then the existing <code>EncryptionConfiguration</code> is
        maintained on the destination.</p> <p>If the destination type is not the
        same, for example, changing the destination from Amazon S3 to Amazon
        Redshift, Kinesis Data Firehose does not merge any parameters. In this
        case, all parameters must be specified.</p> <p>Kinesis Data Firehose
        uses <code>CurrentDeliveryStreamVersionId</code> to avoid race
        conditions and conflicting merges. This is a required field, and the
        service updates the configuration only if the existing configuration has
        a version ID that matches. After the update is applied successfully, the
        version ID is updated, and can be retrieved using
        <a>DescribeDeliveryStream</a>. Use the new version ID to set
        <code>CurrentDeliveryStreamVersionId</code> in the next call.</p>
      responses:
        "200":
          description: Success
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/UpdateDestinationOutput"
        "480":
          description: InvalidArgumentException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/InvalidArgumentException"
        "481":
          description: ResourceInUseException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ResourceInUseException"
        "482":
          description: ResourceNotFoundException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ResourceNotFoundException"
        "483":
          description: ConcurrentModificationException
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ConcurrentModificationException"
      parameters:
        - name: X-Amz-Target
          in: header
          required: true
          schema:
            type: string
            enum:
              - Firehose_20150804.UpdateDestination
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/UpdateDestinationInput"
        required: true
    parameters:
      - $ref: "#/components/parameters/X-Amz-Content-Sha256"
      - $ref: "#/components/parameters/X-Amz-Date"
      - $ref: "#/components/parameters/X-Amz-Algorithm"
      - $ref: "#/components/parameters/X-Amz-Credential"
      - $ref: "#/components/parameters/X-Amz-Security-Token"
      - $ref: "#/components/parameters/X-Amz-Signature"
      - $ref: "#/components/parameters/X-Amz-SignedHeaders"
servers:
  - url: http://firehose.{region}.amazonaws.com
    variables:
      region:
        description: The AWS region
        enum:
          - us-east-1
          - us-east-2
          - us-west-1
          - us-west-2
          - us-gov-west-1
          - us-gov-east-1
          - ca-central-1
          - eu-north-1
          - eu-west-1
          - eu-west-2
          - eu-west-3
          - eu-central-1
          - ap-northeast-1
          - ap-northeast-2
          - ap-northeast-3
          - ap-southeast-1
          - ap-southeast-2
          - ap-south-1
          - sa-east-1
        default: us-east-1
    description: The Firehose multi-region endpoint
  - url: https://firehose.{region}.amazonaws.com
    variables:
      region:
        description: The AWS region
        enum:
          - us-east-1
          - us-east-2
          - us-west-1
          - us-west-2
          - us-gov-west-1
          - us-gov-east-1
          - ca-central-1
          - eu-north-1
          - eu-west-1
          - eu-west-2
          - eu-west-3
          - eu-central-1
          - ap-northeast-1
          - ap-northeast-2
          - ap-northeast-3
          - ap-southeast-1
          - ap-southeast-2
          - ap-south-1
          - sa-east-1
        default: us-east-1
    description: The Firehose multi-region endpoint
  - url: http://firehose.{region}.amazonaws.com.cn
    variables:
      region:
        description: The AWS region
        enum:
          - cn-north-1
          - cn-northwest-1
        default: cn-north-1
    description: The Firehose endpoint for China (Beijing) and China (Ningxia)
  - url: https://firehose.{region}.amazonaws.com.cn
    variables:
      region:
        description: The AWS region
        enum:
          - cn-north-1
          - cn-northwest-1
        default: cn-north-1
    description: The Firehose endpoint for China (Beijing) and China (Ningxia)
components:
  parameters:
    X-Amz-Content-Sha256:
      name: X-Amz-Content-Sha256
      in: header
      required: false
      schema:
        type: string
    X-Amz-Date:
      name: X-Amz-Date
      in: header
      required: false
      schema:
        type: string
    X-Amz-Algorithm:
      name: X-Amz-Algorithm
      in: header
      required: false
      schema:
        type: string
    X-Amz-Credential:
      name: X-Amz-Credential
      in: header
      required: false
      schema:
        type: string
    X-Amz-Security-Token:
      name: X-Amz-Security-Token
      in: header
      required: false
      schema:
        type: string
    X-Amz-Signature:
      name: X-Amz-Signature
      in: header
      required: false
      schema:
        type: string
    X-Amz-SignedHeaders:
      name: X-Amz-SignedHeaders
      in: header
      required: false
      schema:
        type: string
  securitySchemes:
    hmac:
      type: apiKey
      name: Authorization
      in: header
      description: Amazon Signature authorization v4
      x-amazon-apigateway-authtype: awsSigv4
  schemas:
    CreateDeliveryStreamOutput:
      type: object
      properties:
        DeliveryStreamARN:
          $ref: "#/components/schemas/DeliveryStreamARN"
    CreateDeliveryStreamInput:
      type: object
      required:
        - DeliveryStreamName
      properties:
        DeliveryStreamName:
          $ref: "#/components/schemas/DeliveryStreamName"
        DeliveryStreamType:
          $ref: "#/components/schemas/DeliveryStreamType"
        KinesisStreamSourceConfiguration:
          $ref: "#/components/schemas/KinesisStreamSourceConfiguration"
        S3DestinationConfiguration:
          $ref: "#/components/schemas/S3DestinationConfiguration"
        ExtendedS3DestinationConfiguration:
          $ref: "#/components/schemas/ExtendedS3DestinationConfiguration"
        RedshiftDestinationConfiguration:
          $ref: "#/components/schemas/RedshiftDestinationConfiguration"
        ElasticsearchDestinationConfiguration:
          $ref: "#/components/schemas/ElasticsearchDestinationConfiguration"
        SplunkDestinationConfiguration:
          $ref: "#/components/schemas/SplunkDestinationConfiguration"
        Tags:
          $ref: "#/components/schemas/TagDeliveryStreamInputTagList"
    InvalidArgumentException: {}
    LimitExceededException: {}
    ResourceInUseException: {}
    DeleteDeliveryStreamOutput:
      type: object
      properties: {}
    DeleteDeliveryStreamInput:
      type: object
      required:
        - DeliveryStreamName
      properties:
        DeliveryStreamName:
          $ref: "#/components/schemas/DeliveryStreamName"
    ResourceNotFoundException: {}
    DescribeDeliveryStreamOutput:
      type: object
      required:
        - DeliveryStreamDescription
      properties:
        DeliveryStreamDescription:
          $ref: "#/components/schemas/DeliveryStreamDescription"
    DescribeDeliveryStreamInput:
      type: object
      required:
        - DeliveryStreamName
      properties:
        DeliveryStreamName:
          $ref: "#/components/schemas/DeliveryStreamName"
        Limit:
          $ref: "#/components/schemas/DescribeDeliveryStreamInputLimit"
        ExclusiveStartDestinationId:
          $ref: "#/components/schemas/DestinationId"
    ListDeliveryStreamsOutput:
      type: object
      required:
        - DeliveryStreamNames
        - HasMoreDeliveryStreams
      properties:
        DeliveryStreamNames:
          $ref: "#/components/schemas/DeliveryStreamNameList"
        HasMoreDeliveryStreams:
          $ref: "#/components/schemas/BooleanObject"
    ListDeliveryStreamsInput:
      type: object
      properties:
        Limit:
          $ref: "#/components/schemas/ListDeliveryStreamsInputLimit"
        DeliveryStreamType:
          $ref: "#/components/schemas/DeliveryStreamType"
        ExclusiveStartDeliveryStreamName:
          $ref: "#/components/schemas/DeliveryStreamName"
    ListTagsForDeliveryStreamOutput:
      type: object
      required:
        - Tags
        - HasMoreTags
      properties:
        Tags:
          $ref: "#/components/schemas/ListTagsForDeliveryStreamOutputTagList"
        HasMoreTags:
          $ref: "#/components/schemas/BooleanObject"
    ListTagsForDeliveryStreamInput:
      type: object
      required:
        - DeliveryStreamName
      properties:
        DeliveryStreamName:
          $ref: "#/components/schemas/DeliveryStreamName"
        ExclusiveStartTagKey:
          $ref: "#/components/schemas/TagKey"
        Limit:
          $ref: "#/components/schemas/ListTagsForDeliveryStreamInputLimit"
    PutRecordOutput:
      type: object
      required:
        - RecordId
      properties:
        RecordId:
          $ref: "#/components/schemas/PutResponseRecordId"
        Encrypted:
          $ref: "#/components/schemas/BooleanObject"
    PutRecordInput:
      type: object
      required:
        - DeliveryStreamName
        - Record
      properties:
        DeliveryStreamName:
          $ref: "#/components/schemas/DeliveryStreamName"
        Record:
          $ref: "#/components/schemas/Record"
    ServiceUnavailableException: {}
    PutRecordBatchOutput:
      type: object
      required:
        - FailedPutCount
        - RequestResponses
      properties:
        FailedPutCount:
          $ref: "#/components/schemas/NonNegativeIntegerObject"
        Encrypted:
          $ref: "#/components/schemas/BooleanObject"
        RequestResponses:
          $ref: "#/components/schemas/PutRecordBatchResponseEntryList"
    PutRecordBatchInput:
      type: object
      required:
        - DeliveryStreamName
        - Records
      properties:
        DeliveryStreamName:
          $ref: "#/components/schemas/DeliveryStreamName"
        Records:
          $ref: "#/components/schemas/PutRecordBatchRequestEntryList"
    StartDeliveryStreamEncryptionOutput:
      type: object
      properties: {}
    StartDeliveryStreamEncryptionInput:
      type: object
      required:
        - DeliveryStreamName
      properties:
        DeliveryStreamName:
          $ref: "#/components/schemas/DeliveryStreamName"
    StopDeliveryStreamEncryptionOutput:
      type: object
      properties: {}
    StopDeliveryStreamEncryptionInput:
      type: object
      required:
        - DeliveryStreamName
      properties:
        DeliveryStreamName:
          $ref: "#/components/schemas/DeliveryStreamName"
    TagDeliveryStreamOutput:
      type: object
      properties: {}
    TagDeliveryStreamInput:
      type: object
      required:
        - DeliveryStreamName
        - Tags
      properties:
        DeliveryStreamName:
          $ref: "#/components/schemas/DeliveryStreamName"
        Tags:
          $ref: "#/components/schemas/TagDeliveryStreamInputTagList"
    UntagDeliveryStreamOutput:
      type: object
      properties: {}
    UntagDeliveryStreamInput:
      type: object
      required:
        - DeliveryStreamName
        - TagKeys
      properties:
        DeliveryStreamName:
          $ref: "#/components/schemas/DeliveryStreamName"
        TagKeys:
          $ref: "#/components/schemas/TagKeyList"
    UpdateDestinationOutput:
      type: object
      properties: {}
    UpdateDestinationInput:
      type: object
      required:
        - DeliveryStreamName
        - CurrentDeliveryStreamVersionId
        - DestinationId
      properties:
        DeliveryStreamName:
          $ref: "#/components/schemas/DeliveryStreamName"
        CurrentDeliveryStreamVersionId:
          $ref: "#/components/schemas/DeliveryStreamVersionId"
        DestinationId:
          $ref: "#/components/schemas/DestinationId"
        S3DestinationUpdate:
          $ref: "#/components/schemas/S3DestinationUpdate"
        ExtendedS3DestinationUpdate:
          $ref: "#/components/schemas/ExtendedS3DestinationUpdate"
        RedshiftDestinationUpdate:
          $ref: "#/components/schemas/RedshiftDestinationUpdate"
        ElasticsearchDestinationUpdate:
          $ref: "#/components/schemas/ElasticsearchDestinationUpdate"
        SplunkDestinationUpdate:
          $ref: "#/components/schemas/SplunkDestinationUpdate"
    ConcurrentModificationException: {}
    AWSKMSKeyARN:
      type: string
      pattern: arn:.*
      minLength: 1
      maxLength: 512
    BlockSizeBytes:
      type: integer
      minimum: 67108864
    BooleanObject:
      type: boolean
    BucketARN:
      type: string
      pattern: arn:.*
      minLength: 1
      maxLength: 2048
    SizeInMBs:
      type: integer
      minimum: 1
      maximum: 128
    IntervalInSeconds:
      type: integer
      minimum: 60
      maximum: 900
    BufferingHints:
      type: object
      properties:
        SizeInMBs:
          $ref: "#/components/schemas/SizeInMBs"
        IntervalInSeconds:
          $ref: "#/components/schemas/IntervalInSeconds"
      description: Describes hints for the buffering to perform before delivering data to
        the destination. These options are treated as hints, and therefore
        Kinesis Data Firehose might choose to use different values when it is
        optimal.
    LogGroupName:
      type: string
    LogStreamName:
      type: string
    CloudWatchLoggingOptions:
      type: object
      properties:
        Enabled:
          $ref: "#/components/schemas/BooleanObject"
        LogGroupName:
          $ref: "#/components/schemas/LogGroupName"
        LogStreamName:
          $ref: "#/components/schemas/LogStreamName"
      description: Describes the Amazon CloudWatch logging options for your delivery stream.
    ClusterJDBCURL:
      type: string
      pattern: jdbc:(redshift|postgresql)://((?!-)[A-Za-z0-9-]{1,63}(?<!-)\.)+redshift\.amazonaws\.com:\d{1,5}/[a-zA-Z0-9_$]+
      minLength: 1
    NonEmptyString:
      type: string
      pattern: ^(?!\s*$).+
    ColumnToJsonKeyMappings:
      type: object
      additionalProperties:
        $ref: "#/components/schemas/NonEmptyString"
    CompressionFormat:
      type: string
      enum:
        - UNCOMPRESSED
        - GZIP
        - ZIP
        - Snappy
    DataTableName:
      type: string
      minLength: 1
    DataTableColumns:
      type: string
    CopyOptions:
      type: string
    CopyCommand:
      type: object
      required:
        - DataTableName
      properties:
        DataTableName:
          $ref: "#/components/schemas/DataTableName"
        DataTableColumns:
          $ref: "#/components/schemas/DataTableColumns"
        CopyOptions:
          $ref: "#/components/schemas/CopyOptions"
      description: Describes a <code>COPY</code> command for Amazon Redshift.
    DeliveryStreamName:
      type: string
      pattern: "[a-zA-Z0-9_.-]+"
      minLength: 1
      maxLength: 64
    DeliveryStreamType:
      type: string
      enum:
        - DirectPut
        - KinesisStreamAsSource
    KinesisStreamSourceConfiguration:
      type: object
      required:
        - KinesisStreamARN
        - RoleARN
      properties:
        KinesisStreamARN:
          $ref: "#/components/schemas/KinesisStreamARN"
        RoleARN:
          $ref: "#/components/schemas/RoleARN"
      description: The stream and role Amazon Resource Names (ARNs) for a Kinesis data
        stream used as the source for a delivery stream.
    S3DestinationConfiguration:
      type: object
      required:
        - RoleARN
        - BucketARN
      properties:
        RoleARN:
          $ref: "#/components/schemas/RoleARN"
        BucketARN:
          $ref: "#/components/schemas/BucketARN"
        Prefix:
          $ref: "#/components/schemas/Prefix"
        ErrorOutputPrefix:
          $ref: "#/components/schemas/ErrorOutputPrefix"
        BufferingHints:
          $ref: "#/components/schemas/BufferingHints"
        CompressionFormat:
          $ref: "#/components/schemas/CompressionFormat"
        EncryptionConfiguration:
          $ref: "#/components/schemas/EncryptionConfiguration"
        CloudWatchLoggingOptions:
          $ref: "#/components/schemas/CloudWatchLoggingOptions"
      description: Describes the configuration of a destination in Amazon S3.
    ExtendedS3DestinationConfiguration:
      type: object
      required:
        - RoleARN
        - BucketARN
      properties:
        RoleARN:
          $ref: "#/components/schemas/RoleARN"
        BucketARN:
          $ref: "#/components/schemas/BucketARN"
        Prefix:
          $ref: "#/components/schemas/Prefix"
        ErrorOutputPrefix:
          $ref: "#/components/schemas/ErrorOutputPrefix"
        BufferingHints:
          $ref: "#/components/schemas/BufferingHints"
        CompressionFormat:
          $ref: "#/components/schemas/CompressionFormat"
        EncryptionConfiguration:
          $ref: "#/components/schemas/EncryptionConfiguration"
        CloudWatchLoggingOptions:
          $ref: "#/components/schemas/CloudWatchLoggingOptions"
        ProcessingConfiguration:
          $ref: "#/components/schemas/ProcessingConfiguration"
        S3BackupMode:
          $ref: "#/components/schemas/S3BackupMode"
        S3BackupConfiguration:
          $ref: "#/components/schemas/S3DestinationConfiguration"
        DataFormatConversionConfiguration:
          $ref: "#/components/schemas/DataFormatConversionConfiguration"
      description: Describes the configuration of a destination in Amazon S3.
    RedshiftDestinationConfiguration:
      type: object
      required:
        - RoleARN
        - ClusterJDBCURL
        - CopyCommand
        - Username
        - Password
        - S3Configuration
      properties:
        RoleARN:
          $ref: "#/components/schemas/RoleARN"
        ClusterJDBCURL:
          $ref: "#/components/schemas/ClusterJDBCURL"
        CopyCommand:
          $ref: "#/components/schemas/CopyCommand"
        Username:
          $ref: "#/components/schemas/Username"
        Password:
          $ref: "#/components/schemas/Password"
        RetryOptions:
          $ref: "#/components/schemas/RedshiftRetryOptions"
        S3Configuration:
          $ref: "#/components/schemas/S3DestinationConfiguration"
        ProcessingConfiguration:
          $ref: "#/components/schemas/ProcessingConfiguration"
        S3BackupMode:
          $ref: "#/components/schemas/RedshiftS3BackupMode"
        S3BackupConfiguration:
          $ref: "#/components/schemas/S3DestinationConfiguration"
        CloudWatchLoggingOptions:
          $ref: "#/components/schemas/CloudWatchLoggingOptions"
      description: Describes the configuration of a destination in Amazon Redshift.
    ElasticsearchDestinationConfiguration:
      type: object
      required:
        - RoleARN
        - DomainARN
        - IndexName
        - TypeName
        - S3Configuration
      properties:
        RoleARN:
          $ref: "#/components/schemas/RoleARN"
        DomainARN:
          $ref: "#/components/schemas/ElasticsearchDomainARN"
        IndexName:
          $ref: "#/components/schemas/ElasticsearchIndexName"
        TypeName:
          $ref: "#/components/schemas/ElasticsearchTypeName"
        IndexRotationPeriod:
          $ref: "#/components/schemas/ElasticsearchIndexRotationPeriod"
        BufferingHints:
          $ref: "#/components/schemas/ElasticsearchBufferingHints"
        RetryOptions:
          $ref: "#/components/schemas/ElasticsearchRetryOptions"
        S3BackupMode:
          $ref: "#/components/schemas/ElasticsearchS3BackupMode"
        S3Configuration:
          $ref: "#/components/schemas/S3DestinationConfiguration"
        ProcessingConfiguration:
          $ref: "#/components/schemas/ProcessingConfiguration"
        CloudWatchLoggingOptions:
          $ref: "#/components/schemas/CloudWatchLoggingOptions"
      description: Describes the configuration of a destination in Amazon ES.
    SplunkDestinationConfiguration:
      type: object
      required:
        - HECEndpoint
        - HECEndpointType
        - HECToken
        - S3Configuration
      properties:
        HECEndpoint:
          $ref: "#/components/schemas/HECEndpoint"
        HECEndpointType:
          $ref: "#/components/schemas/HECEndpointType"
        HECToken:
          $ref: "#/components/schemas/HECToken"
        HECAcknowledgmentTimeoutInSeconds:
          $ref: "#/components/schemas/HECAcknowledgmentTimeoutInSeconds"
        RetryOptions:
          $ref: "#/components/schemas/SplunkRetryOptions"
        S3BackupMode:
          $ref: "#/components/schemas/SplunkS3BackupMode"
        S3Configuration:
          $ref: "#/components/schemas/S3DestinationConfiguration"
        ProcessingConfiguration:
          $ref: "#/components/schemas/ProcessingConfiguration"
        CloudWatchLoggingOptions:
          $ref: "#/components/schemas/CloudWatchLoggingOptions"
      description: Describes the configuration of a destination in Splunk.
    TagDeliveryStreamInputTagList:
      type: array
      items:
        $ref: "#/components/schemas/Tag"
      minItems: 1
      maxItems: 50
    DeliveryStreamARN:
      type: string
      pattern: arn:.*
      minLength: 1
      maxLength: 512
    Data:
      type: string
      minLength: 0
      maxLength: 1024000
    SchemaConfiguration:
      type: object
      properties:
        RoleARN:
          $ref: "#/components/schemas/NonEmptyStringWithoutWhitespace"
        CatalogId:
          $ref: "#/components/schemas/NonEmptyStringWithoutWhitespace"
        DatabaseName:
          $ref: "#/components/schemas/NonEmptyStringWithoutWhitespace"
        TableName:
          $ref: "#/components/schemas/NonEmptyStringWithoutWhitespace"
        Region:
          $ref: "#/components/schemas/NonEmptyStringWithoutWhitespace"
        VersionId:
          $ref: "#/components/schemas/NonEmptyStringWithoutWhitespace"
      description: Specifies the schema to which you want Kinesis Data Firehose to
        configure your data before it writes it to Amazon S3.
    InputFormatConfiguration:
      type: object
      properties:
        Deserializer:
          $ref: "#/components/schemas/Deserializer"
      description: Specifies the deserializer you want to use to convert the format of the
        input data.
    OutputFormatConfiguration:
      type: object
      properties:
        Serializer:
          $ref: "#/components/schemas/Serializer"
      description: Specifies the serializer that you want Kinesis Data Firehose to use to
        convert the format of your data before it writes it to Amazon S3.
    DataFormatConversionConfiguration:
      type: object
      properties:
        SchemaConfiguration:
          $ref: "#/components/schemas/SchemaConfiguration"
        InputFormatConfiguration:
          $ref: "#/components/schemas/InputFormatConfiguration"
        OutputFormatConfiguration:
          $ref: "#/components/schemas/OutputFormatConfiguration"
        Enabled:
          $ref: "#/components/schemas/BooleanObject"
      description: Specifies that you want Kinesis Data Firehose to convert data from the
        JSON format to the Parquet or ORC format before writing it to Amazon S3.
        Kinesis Data Firehose uses the serializer and deserializer that you
        specify, in addition to the column information from the AWS Glue table,
        to deserialize your input data from JSON and then serialize it to the
        Parquet or ORC format. For more information, see <a
        href="https://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html">Kinesis
        Data Firehose Record Format Conversion</a>.
    DeliveryStartTimestamp:
      type: string
      format: date-time
    DeliveryStreamStatus:
      type: string
      enum:
        - CREATING
        - DELETING
        - ACTIVE
    DeliveryStreamEncryptionConfiguration:
      type: object
      properties:
        Status:
          $ref: "#/components/schemas/DeliveryStreamEncryptionStatus"
      description: Indicates the server-side encryption (SSE) status for the delivery
        stream.
    DeliveryStreamVersionId:
      type: string
      pattern: "[0-9]+"
      minLength: 1
      maxLength: 50
    Timestamp:
      type: string
      format: date-time
    SourceDescription:
      type: object
      properties:
        KinesisStreamSourceDescription:
          $ref: "#/components/schemas/KinesisStreamSourceDescription"
      description: Details about a Kinesis data stream used as the source for a Kinesis
        Data Firehose delivery stream.
    DestinationDescriptionList:
      type: array
      items:
        $ref: "#/components/schemas/DestinationDescription"
    DeliveryStreamDescription:
      type: object
      required:
        - DeliveryStreamName
        - DeliveryStreamARN
        - DeliveryStreamStatus
        - DeliveryStreamType
        - VersionId
        - Destinations
        - HasMoreDestinations
      properties:
        DeliveryStreamName:
          $ref: "#/components/schemas/DeliveryStreamName"
        DeliveryStreamARN:
          $ref: "#/components/schemas/DeliveryStreamARN"
        DeliveryStreamStatus:
          $ref: "#/components/schemas/DeliveryStreamStatus"
        DeliveryStreamEncryptionConfiguration:
          $ref: "#/components/schemas/DeliveryStreamEncryptionConfiguration"
        DeliveryStreamType:
          $ref: "#/components/schemas/DeliveryStreamType"
        VersionId:
          $ref: "#/components/schemas/DeliveryStreamVersionId"
        CreateTimestamp:
          $ref: "#/components/schemas/Timestamp"
        LastUpdateTimestamp:
          $ref: "#/components/schemas/Timestamp"
        Source:
          $ref: "#/components/schemas/SourceDescription"
        Destinations:
          $ref: "#/components/schemas/DestinationDescriptionList"
        HasMoreDestinations:
          $ref: "#/components/schemas/BooleanObject"
      description: Contains information about a delivery stream.
    DeliveryStreamEncryptionStatus:
      type: string
      enum:
        - ENABLED
        - ENABLING
        - DISABLED
        - DISABLING
    DeliveryStreamNameList:
      type: array
      items:
        $ref: "#/components/schemas/DeliveryStreamName"
    DescribeDeliveryStreamInputLimit:
      type: integer
      minimum: 1
      maximum: 10000
    DestinationId:
      type: string
      minLength: 1
      maxLength: 100
    OpenXJsonSerDe:
      type: object
      properties:
        ConvertDotsInJsonKeysToUnderscores:
          $ref: "#/components/schemas/BooleanObject"
        CaseInsensitive:
          $ref: "#/components/schemas/BooleanObject"
        ColumnToJsonKeyMappings:
          $ref: "#/components/schemas/ColumnToJsonKeyMappings"
      description: The OpenX SerDe. Used by Kinesis Data Firehose for deserializing data,
        which means converting it from the JSON format in preparation for
        serializing it to the Parquet or ORC format. This is one of two
        deserializers you can choose, depending on which one offers the
        functionality you need. The other option is the native Hive / HCatalog
        JsonSerDe.
    HiveJsonSerDe:
      type: object
      properties:
        TimestampFormats:
          $ref: "#/components/schemas/ListOfNonEmptyStrings"
      description: The native Hive / HCatalog JsonSerDe. Used by Kinesis Data Firehose for
        deserializing data, which means converting it from the JSON format in
        preparation for serializing it to the Parquet or ORC format. This is one
        of two deserializers you can choose, depending on which one offers the
        functionality you need. The other option is the OpenX SerDe.
    Deserializer:
      type: object
      properties:
        OpenXJsonSerDe:
          $ref: "#/components/schemas/OpenXJsonSerDe"
        HiveJsonSerDe:
          $ref: "#/components/schemas/HiveJsonSerDe"
      description: 'The deserializer you want Kinesis Data Firehose to use for converting
        the input data from JSON. Kinesis Data Firehose then serializes the data
        to its final format using the <a>Serializer</a>. Kinesis Data Firehose
        supports two types of deserializers: the <a
        href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-JSON">Apache
        Hive JSON SerDe</a> and the <a
        href="https://github.com/rcongiu/Hive-JSON-Serde">OpenX JSON SerDe</a>.'
    S3DestinationDescription:
      type: object
      required:
        - RoleARN
        - BucketARN
        - BufferingHints
        - CompressionFormat
        - EncryptionConfiguration
      properties:
        RoleARN:
          $ref: "#/components/schemas/RoleARN"
        BucketARN:
          $ref: "#/components/schemas/BucketARN"
        Prefix:
          $ref: "#/components/schemas/Prefix"
        ErrorOutputPrefix:
          $ref: "#/components/schemas/ErrorOutputPrefix"
        BufferingHints:
          $ref: "#/components/schemas/BufferingHints"
        CompressionFormat:
          $ref: "#/components/schemas/CompressionFormat"
        EncryptionConfiguration:
          $ref: "#/components/schemas/EncryptionConfiguration"
        CloudWatchLoggingOptions:
          $ref: "#/components/schemas/CloudWatchLoggingOptions"
      description: Describes a destination in Amazon S3.
    ExtendedS3DestinationDescription:
      type: object
      required:
        - RoleARN
        - BucketARN
        - BufferingHints
        - CompressionFormat
        - EncryptionConfiguration
      properties:
        RoleARN:
          $ref: "#/components/schemas/RoleARN"
        BucketARN:
          $ref: "#/components/schemas/BucketARN"
        Prefix:
          $ref: "#/components/schemas/Prefix"
        ErrorOutputPrefix:
          $ref: "#/components/schemas/ErrorOutputPrefix"
        BufferingHints:
          $ref: "#/components/schemas/BufferingHints"
        CompressionFormat:
          $ref: "#/components/schemas/CompressionFormat"
        EncryptionConfiguration:
          $ref: "#/components/schemas/EncryptionConfiguration"
        CloudWatchLoggingOptions:
          $ref: "#/components/schemas/CloudWatchLoggingOptions"
        ProcessingConfiguration:
          $ref: "#/components/schemas/ProcessingConfiguration"
        S3BackupMode:
          $ref: "#/components/schemas/S3BackupMode"
        S3BackupDescription:
          $ref: "#/components/schemas/S3DestinationDescription"
        DataFormatConversionConfiguration:
          $ref: "#/components/schemas/DataFormatConversionConfiguration"
      description: Describes a destination in Amazon S3.
    RedshiftDestinationDescription:
      type: object
      required:
        - RoleARN
        - ClusterJDBCURL
        - CopyCommand
        - Username
        - S3DestinationDescription
      properties:
        RoleARN:
          $ref: "#/components/schemas/RoleARN"
        ClusterJDBCURL:
          $ref: "#/components/schemas/ClusterJDBCURL"
        CopyCommand:
          $ref: "#/components/schemas/CopyCommand"
        Username:
          $ref: "#/components/schemas/Username"
        RetryOptions:
          $ref: "#/components/schemas/RedshiftRetryOptions"
        S3DestinationDescription:
          $ref: "#/components/schemas/S3DestinationDescription"
        ProcessingConfiguration:
          $ref: "#/components/schemas/ProcessingConfiguration"
        S3BackupMode:
          $ref: "#/components/schemas/RedshiftS3BackupMode"
        S3BackupDescription:
          $ref: "#/components/schemas/S3DestinationDescription"
        CloudWatchLoggingOptions:
          $ref: "#/components/schemas/CloudWatchLoggingOptions"
      description: Describes a destination in Amazon Redshift.
    ElasticsearchDestinationDescription:
      type: object
      properties:
        RoleARN:
          $ref: "#/components/schemas/RoleARN"
        DomainARN:
          $ref: "#/components/schemas/ElasticsearchDomainARN"
        IndexName:
          $ref: "#/components/schemas/ElasticsearchIndexName"
        TypeName:
          $ref: "#/components/schemas/ElasticsearchTypeName"
        IndexRotationPeriod:
          $ref: "#/components/schemas/ElasticsearchIndexRotationPeriod"
        BufferingHints:
          $ref: "#/components/schemas/ElasticsearchBufferingHints"
        RetryOptions:
          $ref: "#/components/schemas/ElasticsearchRetryOptions"
        S3BackupMode:
          $ref: "#/components/schemas/ElasticsearchS3BackupMode"
        S3DestinationDescription:
          $ref: "#/components/schemas/S3DestinationDescription"
        ProcessingConfiguration:
          $ref: "#/components/schemas/ProcessingConfiguration"
        CloudWatchLoggingOptions:
          $ref: "#/components/schemas/CloudWatchLoggingOptions"
      description: The destination description in Amazon ES.
    SplunkDestinationDescription:
      type: object
      properties:
        HECEndpoint:
          $ref: "#/components/schemas/HECEndpoint"
        HECEndpointType:
          $ref: "#/components/schemas/HECEndpointType"
        HECToken:
          $ref: "#/components/schemas/HECToken"
        HECAcknowledgmentTimeoutInSeconds:
          $ref: "#/components/schemas/HECAcknowledgmentTimeoutInSeconds"
        RetryOptions:
          $ref: "#/components/schemas/SplunkRetryOptions"
        S3BackupMode:
          $ref: "#/components/schemas/SplunkS3BackupMode"
        S3DestinationDescription:
          $ref: "#/components/schemas/S3DestinationDescription"
        ProcessingConfiguration:
          $ref: "#/components/schemas/ProcessingConfiguration"
        CloudWatchLoggingOptions:
          $ref: "#/components/schemas/CloudWatchLoggingOptions"
      description: Describes a destination in Splunk.
    DestinationDescription:
      type: object
      required:
        - DestinationId
      properties:
        DestinationId:
          $ref: "#/components/schemas/DestinationId"
        S3DestinationDescription:
          $ref: "#/components/schemas/S3DestinationDescription"
        ExtendedS3DestinationDescription:
          $ref: "#/components/schemas/ExtendedS3DestinationDescription"
        RedshiftDestinationDescription:
          $ref: "#/components/schemas/RedshiftDestinationDescription"
        ElasticsearchDestinationDescription:
          $ref: "#/components/schemas/ElasticsearchDestinationDescription"
        SplunkDestinationDescription:
          $ref: "#/components/schemas/SplunkDestinationDescription"
      description: Describes the destination for a delivery stream.
    ElasticsearchBufferingIntervalInSeconds:
      type: integer
      minimum: 60
      maximum: 900
    ElasticsearchBufferingSizeInMBs:
      type: integer
      minimum: 1
      maximum: 100
    ElasticsearchBufferingHints:
      type: object
      properties:
        IntervalInSeconds:
          $ref: "#/components/schemas/ElasticsearchBufferingIntervalInSeconds"
        SizeInMBs:
          $ref: "#/components/schemas/ElasticsearchBufferingSizeInMBs"
      description: Describes the buffering to perform before delivering data to the Amazon
        ES destination.
    RoleARN:
      type: string
      pattern: arn:.*
      minLength: 1
      maxLength: 512
    ElasticsearchDomainARN:
      type: string
      pattern: arn:.*
      minLength: 1
      maxLength: 512
    ElasticsearchIndexName:
      type: string
      minLength: 1
      maxLength: 80
    ElasticsearchTypeName:
      type: string
      minLength: 1
      maxLength: 100
    ElasticsearchIndexRotationPeriod:
      type: string
      enum:
        - NoRotation
        - OneHour
        - OneDay
        - OneWeek
        - OneMonth
    ElasticsearchRetryOptions:
      type: object
      properties:
        DurationInSeconds:
          $ref: "#/components/schemas/ElasticsearchRetryDurationInSeconds"
      description: Configures retry behavior in case Kinesis Data Firehose is unable to
        deliver documents to Amazon ES.
    ElasticsearchS3BackupMode:
      type: string
      enum:
        - FailedDocumentsOnly
        - AllDocuments
    ProcessingConfiguration:
      type: object
      properties:
        Enabled:
          $ref: "#/components/schemas/BooleanObject"
        Processors:
          $ref: "#/components/schemas/ProcessorList"
      description: Describes a data processing configuration.
    S3DestinationUpdate:
      type: object
      properties:
        RoleARN:
          $ref: "#/components/schemas/RoleARN"
        BucketARN:
          $ref: "#/components/schemas/BucketARN"
        Prefix:
          $ref: "#/components/schemas/Prefix"
        ErrorOutputPrefix:
          $ref: "#/components/schemas/ErrorOutputPrefix"
        BufferingHints:
          $ref: "#/components/schemas/BufferingHints"
        CompressionFormat:
          $ref: "#/components/schemas/CompressionFormat"
        EncryptionConfiguration:
          $ref: "#/components/schemas/EncryptionConfiguration"
        CloudWatchLoggingOptions:
          $ref: "#/components/schemas/CloudWatchLoggingOptions"
      description: Describes an update for a destination in Amazon S3.
    ElasticsearchDestinationUpdate:
      type: object
      properties:
        RoleARN:
          $ref: "#/components/schemas/RoleARN"
        DomainARN:
          $ref: "#/components/schemas/ElasticsearchDomainARN"
        IndexName:
          $ref: "#/components/schemas/ElasticsearchIndexName"
        TypeName:
          $ref: "#/components/schemas/ElasticsearchTypeName"
        IndexRotationPeriod:
          $ref: "#/components/schemas/ElasticsearchIndexRotationPeriod"
        BufferingHints:
          $ref: "#/components/schemas/ElasticsearchBufferingHints"
        RetryOptions:
          $ref: "#/components/schemas/ElasticsearchRetryOptions"
        S3Update:
          $ref: "#/components/schemas/S3DestinationUpdate"
        ProcessingConfiguration:
          $ref: "#/components/schemas/ProcessingConfiguration"
        CloudWatchLoggingOptions:
          $ref: "#/components/schemas/CloudWatchLoggingOptions"
      description: Describes an update for a destination in Amazon ES.
    ElasticsearchRetryDurationInSeconds:
      type: integer
      minimum: 0
      maximum: 7200
    NoEncryptionConfig:
      type: string
      enum:
        - NoEncryption
    KMSEncryptionConfig:
      type: object
      required:
        - AWSKMSKeyARN
      properties:
        AWSKMSKeyARN:
          $ref: "#/components/schemas/AWSKMSKeyARN"
      description: Describes an encryption key for a destination in Amazon S3.
    EncryptionConfiguration:
      type: object
      properties:
        NoEncryptionConfig:
          $ref: "#/components/schemas/NoEncryptionConfig"
        KMSEncryptionConfig:
          $ref: "#/components/schemas/KMSEncryptionConfig"
      description: Describes the encryption for a destination in Amazon S3.
    ErrorCode:
      type: string
    ErrorMessage:
      type: string
    ErrorOutputPrefix:
      type: string
    Prefix:
      type: string
    S3BackupMode:
      type: string
      enum:
        - Disabled
        - Enabled
    ExtendedS3DestinationUpdate:
      type: object
      properties:
        RoleARN:
          $ref: "#/components/schemas/RoleARN"
        BucketARN:
          $ref: "#/components/schemas/BucketARN"
        Prefix:
          $ref: "#/components/schemas/Prefix"
        ErrorOutputPrefix:
          $ref: "#/components/schemas/ErrorOutputPrefix"
        BufferingHints:
          $ref: "#/components/schemas/BufferingHints"
        CompressionFormat:
          $ref: "#/components/schemas/CompressionFormat"
        EncryptionConfiguration:
          $ref: "#/components/schemas/EncryptionConfiguration"
        CloudWatchLoggingOptions:
          $ref: "#/components/schemas/CloudWatchLoggingOptions"
        ProcessingConfiguration:
          $ref: "#/components/schemas/ProcessingConfiguration"
        S3BackupMode:
          $ref: "#/components/schemas/S3BackupMode"
        S3BackupUpdate:
          $ref: "#/components/schemas/S3DestinationUpdate"
        DataFormatConversionConfiguration:
          $ref: "#/components/schemas/DataFormatConversionConfiguration"
      description: Describes an update for a destination in Amazon S3.
    HECAcknowledgmentTimeoutInSeconds:
      type: integer
      minimum: 180
      maximum: 600
    HECEndpoint:
      type: string
    HECEndpointType:
      type: string
      enum:
        - Raw
        - Event
    HECToken:
      type: string
    ListOfNonEmptyStrings:
      type: array
      items:
        $ref: "#/components/schemas/NonEmptyString"
    KinesisStreamARN:
      type: string
      pattern: arn:.*
      minLength: 1
      maxLength: 512
    KinesisStreamSourceDescription:
      type: object
      properties:
        KinesisStreamARN:
          $ref: "#/components/schemas/KinesisStreamARN"
        RoleARN:
          $ref: "#/components/schemas/RoleARN"
        DeliveryStartTimestamp:
          $ref: "#/components/schemas/DeliveryStartTimestamp"
      description: Details about a Kinesis data stream used as the source for a Kinesis
        Data Firehose delivery stream.
    ListDeliveryStreamsInputLimit:
      type: integer
      minimum: 1
      maximum: 10000
    NonEmptyStringWithoutWhitespace:
      type: string
      pattern: ^\S+$
    ListOfNonEmptyStringsWithoutWhitespace:
      type: array
      items:
        $ref: "#/components/schemas/NonEmptyStringWithoutWhitespace"
    TagKey:
      type: string
      minLength: 1
      maxLength: 128
    ListTagsForDeliveryStreamInputLimit:
      type: integer
      minimum: 1
      maximum: 50
    ListTagsForDeliveryStreamOutputTagList:
      type: array
      items:
        $ref: "#/components/schemas/Tag"
      minItems: 0
      maxItems: 50
    Tag:
      type: object
      required:
        - Key
      properties:
        Key:
          $ref: "#/components/schemas/TagKey"
        Value:
          $ref: "#/components/schemas/TagValue"
      description: Metadata that you can assign to a delivery stream, consisting of a
        key-value pair.
    NonNegativeIntegerObject:
      type: integer
      minimum: 0
    OrcCompression:
      type: string
      enum:
        - NONE
        - ZLIB
        - SNAPPY
    OrcFormatVersion:
      type: string
      enum:
        - V0_11
        - V0_12
    OrcRowIndexStride:
      type: integer
      minimum: 1000
    OrcStripeSizeBytes:
      type: integer
      minimum: 8388608
    Proportion:
      type: number
      format: double
      minimum: 0
      maximum: 1
    OrcSerDe:
      type: object
      properties:
        StripeSizeBytes:
          $ref: "#/components/schemas/OrcStripeSizeBytes"
        BlockSizeBytes:
          $ref: "#/components/schemas/BlockSizeBytes"
        RowIndexStride:
          $ref: "#/components/schemas/OrcRowIndexStride"
        EnablePadding:
          $ref: "#/components/schemas/BooleanObject"
        PaddingTolerance:
          $ref: "#/components/schemas/Proportion"
        Compression:
          $ref: "#/components/schemas/OrcCompression"
        BloomFilterColumns:
          $ref: "#/components/schemas/ListOfNonEmptyStringsWithoutWhitespace"
        BloomFilterFalsePositiveProbability:
          $ref: "#/components/schemas/Proportion"
        DictionaryKeyThreshold:
          $ref: "#/components/schemas/Proportion"
        FormatVersion:
          $ref: "#/components/schemas/OrcFormatVersion"
      description: A serializer to use for converting data to the ORC format before storing
        it in Amazon S3. For more information, see <a
        href="https://orc.apache.org/docs/">Apache ORC</a>.
    Serializer:
      type: object
      properties:
        ParquetSerDe:
          $ref: "#/components/schemas/ParquetSerDe"
        OrcSerDe:
          $ref: "#/components/schemas/OrcSerDe"
      description: 'The serializer that you want Kinesis Data Firehose to use to convert
        data to the target format before writing it to Amazon S3. Kinesis Data
        Firehose supports two types of serializers: the <a
        href="https://hive.apache.org/javadocs/r1.2.2/api/org/apache/hadoop/hive/ql/io/orc/OrcSerde.html">ORC
        SerDe</a> and the <a
        href="https://hive.apache.org/javadocs/r1.2.2/api/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.html">Parquet
        SerDe</a>.'
    ParquetCompression:
      type: string
      enum:
        - UNCOMPRESSED
        - GZIP
        - SNAPPY
    ParquetPageSizeBytes:
      type: integer
      minimum: 65536
    ParquetWriterVersion:
      type: string
      enum:
        - V1
        - V2
    ParquetSerDe:
      type: object
      properties:
        BlockSizeBytes:
          $ref: "#/components/schemas/BlockSizeBytes"
        PageSizeBytes:
          $ref: "#/components/schemas/ParquetPageSizeBytes"
        Compression:
          $ref: "#/components/schemas/ParquetCompression"
        EnableDictionaryCompression:
          $ref: "#/components/schemas/BooleanObject"
        MaxPaddingBytes:
          $ref: "#/components/schemas/NonNegativeIntegerObject"
        WriterVersion:
          $ref: "#/components/schemas/ParquetWriterVersion"
      description: A serializer to use for converting data to the Parquet format before
        storing it in Amazon S3. For more information, see <a
        href="https://parquet.apache.org/documentation/latest/">Apache
        Parquet</a>.
    Password:
      type: string
      minLength: 6
      format: password
    ProcessorList:
      type: array
      items:
        $ref: "#/components/schemas/Processor"
    ProcessorType:
      type: string
      enum:
        - Lambda
    ProcessorParameterList:
      type: array
      items:
        $ref: "#/components/schemas/ProcessorParameter"
    Processor:
      type: object
      required:
        - Type
      properties:
        Type:
          $ref: "#/components/schemas/ProcessorType"
        Parameters:
          $ref: "#/components/schemas/ProcessorParameterList"
      description: Describes a data processor.
    ProcessorParameterName:
      type: string
      enum:
        - LambdaArn
        - NumberOfRetries
        - RoleArn
        - BufferSizeInMBs
        - BufferIntervalInSeconds
    ProcessorParameterValue:
      type: string
      minLength: 1
      maxLength: 512
    ProcessorParameter:
      type: object
      required:
        - ParameterName
        - ParameterValue
      properties:
        ParameterName:
          $ref: "#/components/schemas/ProcessorParameterName"
        ParameterValue:
          $ref: "#/components/schemas/ProcessorParameterValue"
      description: Describes the processor parameter.
    PutRecordBatchRequestEntryList:
      type: array
      items:
        $ref: "#/components/schemas/Record"
      minItems: 1
      maxItems: 500
    PutRecordBatchResponseEntryList:
      type: array
      items:
        $ref: "#/components/schemas/PutRecordBatchResponseEntry"
      minItems: 1
      maxItems: 500
    Record:
      type: object
      required:
        - Data
      properties:
        Data:
          $ref: "#/components/schemas/Data"
      description: The unit of data in a delivery stream.
    PutResponseRecordId:
      type: string
      minLength: 1
    PutRecordBatchResponseEntry:
      type: object
      properties:
        RecordId:
          $ref: "#/components/schemas/PutResponseRecordId"
        ErrorCode:
          $ref: "#/components/schemas/ErrorCode"
        ErrorMessage:
          $ref: "#/components/schemas/ErrorMessage"
      description: Contains the result for an individual record from a
        <a>PutRecordBatch</a> request. If the record is successfully added to
        your delivery stream, it receives a record ID. If the record fails to be
        added to your delivery stream, the result includes an error code and an
        error message.
    Username:
      type: string
      minLength: 1
      format: password
    RedshiftRetryOptions:
      type: object
      properties:
        DurationInSeconds:
          $ref: "#/components/schemas/RedshiftRetryDurationInSeconds"
      description: Configures retry behavior in case Kinesis Data Firehose is unable to
        deliver documents to Amazon Redshift.
    RedshiftS3BackupMode:
      type: string
      enum:
        - Disabled
        - Enabled
    RedshiftDestinationUpdate:
      type: object
      properties:
        RoleARN:
          $ref: "#/components/schemas/RoleARN"
        ClusterJDBCURL:
          $ref: "#/components/schemas/ClusterJDBCURL"
        CopyCommand:
          $ref: "#/components/schemas/CopyCommand"
        Username:
          $ref: "#/components/schemas/Username"
        Password:
          $ref: "#/components/schemas/Password"
        RetryOptions:
          $ref: "#/components/schemas/RedshiftRetryOptions"
        S3Update:
          $ref: "#/components/schemas/S3DestinationUpdate"
        ProcessingConfiguration:
          $ref: "#/components/schemas/ProcessingConfiguration"
        S3BackupMode:
          $ref: "#/components/schemas/RedshiftS3BackupMode"
        S3BackupUpdate:
          $ref: "#/components/schemas/S3DestinationUpdate"
        CloudWatchLoggingOptions:
          $ref: "#/components/schemas/CloudWatchLoggingOptions"
      description: Describes an update for a destination in Amazon Redshift.
    RedshiftRetryDurationInSeconds:
      type: integer
      minimum: 0
      maximum: 7200
    SplunkRetryOptions:
      type: object
      properties:
        DurationInSeconds:
          $ref: "#/components/schemas/SplunkRetryDurationInSeconds"
      description: Configures retry behavior in case Kinesis Data Firehose is unable to
        deliver documents to Splunk, or if it doesn't receive an acknowledgment
        from Splunk.
    SplunkS3BackupMode:
      type: string
      enum:
        - FailedEventsOnly
        - AllEvents
    SplunkDestinationUpdate:
      type: object
      properties:
        HECEndpoint:
          $ref: "#/components/schemas/HECEndpoint"
        HECEndpointType:
          $ref: "#/components/schemas/HECEndpointType"
        HECToken:
          $ref: "#/components/schemas/HECToken"
        HECAcknowledgmentTimeoutInSeconds:
          $ref: "#/components/schemas/HECAcknowledgmentTimeoutInSeconds"
        RetryOptions:
          $ref: "#/components/schemas/SplunkRetryOptions"
        S3BackupMode:
          $ref: "#/components/schemas/SplunkS3BackupMode"
        S3Update:
          $ref: "#/components/schemas/S3DestinationUpdate"
        ProcessingConfiguration:
          $ref: "#/components/schemas/ProcessingConfiguration"
        CloudWatchLoggingOptions:
          $ref: "#/components/schemas/CloudWatchLoggingOptions"
      description: Describes an update for a destination in Splunk.
    SplunkRetryDurationInSeconds:
      type: integer
      minimum: 0
      maximum: 7200
    TagValue:
      type: string
      minLength: 0
      maxLength: 256
    TagKeyList:
      type: array
      items:
        $ref: "#/components/schemas/TagKey"
      minItems: 1
      maxItems: 50
